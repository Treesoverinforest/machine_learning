============================================================
ЗАДАЧА 1: СЛУЧАЙНЫЙ ЛЕС
============================================================

1.1 Исследование зависимости качества от глубины деревьев
Оптимальная глубина деревьев: 18
Максимальный F1-score: 0.6667

1.2 Исследование зависимости качества от количества признаков на дерево
Оптимальное количество признаков на дерево: 2
Максимальный F1-score: 0.6667

1.3 Исследование зависимости качества и времени обучения от числа деревьев
Оптимальное количество деревьев: 25
Максимальный F1-score: 0.7000
Время обучения для оптимальной модели: 0.07 сек

Финальная модель случайного леса:
F1-score: 0.7000
Accuracy: 0.8052
Precision: 0.7609
Recall: 0.6481
Время обучения: 0.06 сек

Важность признаков (Случайный лес):
Glucose: 0.2551
BMI: 0.1653
DiabetesPedigreeFunction: 0.1340
Age: 0.1176
BloodPressure: 0.0861
Pregnancies: 0.0859
Insulin: 0.0855
SkinThickness: 0.0705

============================================================
ЗАДАЧА 2: XGBOOST
============================================================

2.1 Исследование зависимости качества от max_depth в XGBoost
Оптимальная глубина деревьев для XGBoost: 3
Максимальный F1-score: 0.6731

2.2 Исследование зависимости качества от learning_rate в XGBoost
Оптимальный learning_rate для XGBoost: 0.1
Максимальный F1-score: 0.6731

2.3 Исследование зависимости качества от subsample в XGBoost
Оптимальный subsample для XGBoost: 1.0
Максимальный F1-score: 0.6731

2.4 Исследование зависимости качества и времени обучения от n_estimators в XGBoost
Оптимальное количество деревьев для XGBoost: 100
Максимальный F1-score: 0.6731
Время обучения для оптимальной модели: 0.04 сек

Финальная модель XGBoost:
F1-score: 0.6731
Accuracy: 0.7792
Precision: 0.7000
Recall: 0.6481
Время обучения: 0.04 сек

Важность признаков (XGBoost):
Glucose: 0.3443
BMI: 0.1525
Age: 0.1323
Insulin: 0.0886
DiabetesPedigreeFunction: 0.0832
Pregnancies: 0.0829
SkinThickness: 0.0599
BloodPressure: 0.0563


СРАВНЕНИЕ МОДЕЛЕЙ

       Модель  F1-score  Accuracy  Precision   Recall  Время обучения (сек)
Случайный лес  0.700000  0.805195    0.76087 0.648148              0.059973
      XGBoost  0.673077  0.779221    0.70000 0.648148              0.041562


ВЫВОДЫ 

1. Случайный лес:
   - Оптимальные параметры: глубина=18, признаков на дерево=2, деревьев=25
   - Лучший F1-score: 0.7000
   - Время обучения: 0.06 сек

2. XGBoost:
   - Оптимальные параметры: глубина=3, learning_rate=0.1, subsample=1.0, деревьев=100
   - Лучший F1-score: 0.6731
   - Время обучения: 0.04 сек

3. Сравнение моделей:
   - Лучшее качество (F1-score): Случайный лес (разница: 0.0269)
   - Быстрее обучается: XGBoost (разница: 0.02 сек)

4. Общие выводы:
   - XGBoost обычно показывает немного лучшие результаты по качеству классификации благодаря
     градиентному бустингу, который последовательно исправляет ошибки предыдущих моделей.
   - Случайный лес более устойчив к переобучению благодаря параллельному обучению деревьев
     на разных подвыборках данных и признаков.
   - XGBoost требует более тщательной настройки гиперпараметров (learning_rate, subsample и др.),
     но при правильной настройке может превзойти случайный лес по качеству.
   - Время обучения XGBoost обычно больше, так как деревья обучаются последовательно,
     в то время как в случайном лесу деревья могут обучаться параллельно.
   - Для данного датасета диабета обе модели показали сопоставимые результаты
